<!DOCTYPE html><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Barlow:wght@200;400&family=Bitter&family=Inconsolata&display=swap" rel=stylesheet><link rel=icon type=image/svg+xml href=/notcodinglogo-filled.svg><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/atom-one-light.css><meta name=viewport content="width=device-width,initial-scale=1"><title>Google is massively ahead in the AI battle of 2024 //notcoding.today</title><header><img src=/notcodingtoday.svg alt=//notcoding.today><hr><div class=navigation><a href=/about>About</a> <a href=/ >Blog</a></div><hr></header><div class=blog><h1>Google is massively ahead in the AI battle of 2024</h1><div class=center>2024-03-23</div><h2 id=background>Background</h2><p>At the time of writing, AI battle is heating up and OpenAI has disrupted Google for the first time in their history. The Search-Goliath is at all-time low-popularity across the media and people are cheering for David in this battle.<p>Despite the disruptions, it is very likely that Google will be the one who will win this battle. This is because of the dynamics outside the ML technology itself.<h2 id=openai-and-microsofts-detachment-risk-management>OpenAI and Microsoft&#39;s detachment: risk management</h2><p>There are multiple reasons why Microsoft is not making OpenAI part of Microsoft (unlike what Google did with DeepMind) - MS ultimately does not want to be <em>responsible</em> for the technology and the generated contents.<p>Currently, nothing can mathematically prove that the output of the content will be harmless. Even with rigorous testing, we lack the historic experience to predict mishaps.<p>This leads to the core the issue: when something goes wrong, an actual entity (not &quot;AI&quot;) needs to be responsible for the outcome. In order for the tool-provider (ie. someone who wants to monetize their model) to detach themselves from responsibility, they need to set boundaries on what the tool is meant to do and ensure the tool is not &#39;defective&#39;.<p>If model creator heavily restricts the models&#39; capabilities to mitigate risks, the model loses its ability to solve nuanced and difficult problems. Even adding such limitations cannot guarantee that the model will not misbehave. Most importantly, there are not enough past cases to predict what the legal consequences might be when AI models misbehave, hence the risk is multiplied.<h2 id=aircanada-taking-actual-responsibility>AirCanada taking actual responsibility</h2><p><a href=https://www.washingtonpost.com/travel/2024/02/18/air-canada-airline-chatbot-ruling/ >Air Canada had to honour the discount made up by the AI chat bot to the user</a>. This proves that humans <em>need</em> an entity to take responsibility of LLM&#39;s outcome. This was a light-hearted case where nobody seriously injured, but what if it was much worse? What if an innocent life was lost?<p>Microsoft obviously do not want this headache and is extremely careful approaching the industry. They are invested, but clearly drawing the boundaries. Everything is proxied via OpenAI and branded with non-traditional ways - &quot;Copilot&quot; (with emphasis on the brand that <strong>you</strong> are still the pilot). I think this is a smart move that fulfills my personal risk tolerance.<h2 id=google-the-bold-one>Google, the bold one</h2><p>Google managed to put their Gemini behind <a href=https://blog.google/products/android/google-ai-samsung-galaxy-s24/ >Samsung&#39;s devices</a> and at the time of writing, soon <a href=https://www.forbes.com/sites/siladityaray/2024/03/18/apple-reportedly-in-talks-with-google-to-integrate-geminis-ai-service-into-the-iphone>Apple&#39;s as well</a>.<p>The branding in Samsung&#39;s &quot;Galaxy AI&quot; is still using the Gemini logo, with the disclaimer &quot;Samsung does not make any promises, assurances or guarantees as to the accuracy, completeness or reliability of the output provided by AI features.&quot;. Although I have no information, I can guarantee that Apple will have a similar term.<p>However, in the current <a href=https://ai.google.dev/terms>Google AI Terms of Service</a>, it does not explicitly deny responsibility of all generated content. However, it guards itself against certain scenarios and industries which sounds more vulnerable:<ul><li>&quot;You may not use the Services in clinical practice, to provide medical advice, or in any manner that is overseen by or requires clearance or approval from a medical device regulatory agency.&quot;<li>&quot;You may not attempt to bypass these protective measures or use content that violates the API Terms or these Additional Terms&quot;<li>&quot;You may not use the Services in clinical practice, to provide medical advice, or in any manner that is overseen by or requires clearance or approval from a medical device regulatory agency.&quot;<li>&quot;Don&#39;t rely on the Services for medical, legal, financial, or other professional advice.&quot;<li>&quot;The Services use experimental technology and may sometimes provide inaccurate or offensive content that doesn&#39;t represent Google&#39;s views.&quot;</ul><h3 id=why-is-google-willing-to-take-this-risk>Why is Google willing to take this risk?</h3><p>It seems like Google is confident with its ability around model &#39;safety&#39; during training. They may also have established relationship with various local governments to mitigate risks. I also would not be surprised if engineers from Google and partners behind the scenes are working very closely together to limit the LLM applications to control the risk.<p>I have an uneducated opinion that if generated content is highly personalized, lawsuit/PR risk reduces astronomically:<ul><li>In the event of generated content doing harm, if Google can prove that the generated content was highly tailored, it may have grounds to argue that outcome was self-inflicted by the user. This will hand over the responsibility to the user.<li>If the model&#39;s &quot;opinions&quot; are tailored, then nobody can argue the model is politically biased.</ul><p>I cannot name any other company that has enough user information and behavioural analysis to achieve this.<h2 id=video-generation-and-youtube>Video generation and YouTube</h2><p>I think real generated content war will start with videos (nobody likes reading wall of text, ironic for this blog).<p><a href=https://openai.com/sora>OpenAI Sora</a> is jaw-dropping (probably trained using Unreal Engine renders), but the party trick of nice visuals is insufficient. Just like video games, fancy graphics is not what keeps users hooked. It is the content inside it.<p>What will make users use video generation will be ability to generate good <em>content</em> - the formula that YouTubers have to keep the audience engaged for a reasonable period of time for monetization.<p>This means if we want user-generated videos with very little (critical for UX) input, something in the generation chain needs to understand what is makes a good and addictive video to watch for the user.<p>Google already has monopoly in this industry with YouTube, and has access to incredible amounts of video data that no other entity does. Google is at a massive head start in this AI race.<h2 id=what-if-google-has-a-monopoly-again>What if Google has a monopoly, again</h2><p>I think we, as consumers, want competition in the market and do not want mono/duopolies.<p>Ultimately, the first big winner of AI era would be the entity who can actually make sizable money by using new tech, solving a problem that was not possible before.<p>There is a descent chance that Google&#39;s side (Samsung, Apple and Google itself) will be the first to achieve highly profitable and sustainable AI product, it almost seems inevitable. Then Google will have access to data and information on <em>what</em> makes a profitable model and the industry will start revolving around this benchmark/fact - and the centre of this will be Google.<p>However, normal people and companies have something that G&#39;s family cannot execute in this race - agility to adapt. The only way to prevent upcoming monopoly is to beat them in this race by creating a successful product utilizing new tech first, then &quot;we&quot; get to define what makes a profitable model.<p>In short, startups need to <strong>stop</strong> focusing on model pre-training, but instead, work on fine-tuning and building products that will change the world with the new tech.</div><footer><hr><p>2020-2024 Copyright. All Rights Reserved.</footer>